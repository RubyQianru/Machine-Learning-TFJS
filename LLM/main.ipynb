{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 23:02:42.078480: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# Install the most re version of TensorFlow to use the improved\n",
    "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
    "# %pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text -y\n",
    "# %pip install protobuf~=3.20.3\n",
    "# %pip install -q tensorflow_datasets\n",
    "# %pip install -q -U tensorflow-text tensorflow\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"Good evening my love! ‚ù§Ô∏è How difficult it was to leave my cute and cuddly Pepe Queen this morning, wanting to crawl under the covers and KISS KISS her üòò‚ò∫Ô∏è Her body still remains in my bed as she has shed her outfit in the same way that a snake sheds its skin ü§≠ Today was an incredibly productive day of meetings, to the point where I have almost lost my voice üòã Out of the conversations, one may even already lead to an investment I may get personal access to following the new strategy discussed yesterday night. Today has also marked a significantly positive turn in the market, with Bitcoin up an unbelievable 10% in 24 hours. I am sitting patiently upon my tiny digital empire, watching humbly as my land blossoms and the world is quiet to my knowledge and secrets ü§´ I will update you more tomorrow - goodnight my dear üò¥üíì\",\n",
    "    \"Happy Saturday my beautiful darling! ü•π Another crisp and sunny winter day to spend indoors together in front of some big screens and each other ‚ò∫Ô∏è There's not much to share on my cousin's front, other than they had been together since 2018 which is likely around the last time I saw him, and she is from a tiny town in the middle of nowhere over by him in Michigan. I did bring up one topic that made her have an outsized reaction and roll her eyes which was whether he wanted to move from Michigan to somewhere else ü§≠ I will kiss kiss üòò and banana banana üçå you soon! üò≥ü´£\",\n",
    "    \"Awwwwww\",\n",
    "    \"I miss you!\",\n",
    "    \"Happy Friyay Love! ‚ù§Ô∏è Enjoy your relaxing day at home filled with lots of destressing and watching of friends ü§ó It would be wonderful to try out the special egg yolk one because I am addicted to being the super special person in your life ü§≠ You also have a card from valentines that I hid in our shared email inbox for you to open wuuuuu üòñ See you tomorrow my Beebee and be prepared for lots more feelings of being loved! üòòüêù\",\n",
    "    \"I've spotted our first day with a temperature in the 60s coming Wednesday üò≤ I won't speak too soon but Mr. Snow may be gone for the rest of 2024 üòé How nice the dream of living in Miami feels for half of the year up in the north! We can fix up your helium mobile this weekend and get you back in mapping action earning that stable income source for the family ü§≠ I'll be meeting with my cousin in the afternoon for lunch tomorrow rather than what I originally thought would be dinner. And I also will get a chance to meet his fiance too so I suppose I will have to be on my best behavior and see if there is tea to extract üòÆ‚Äçüí® If you would like to stay over tomorrow after your class since I'll be gone next weekend, that is an option! But up to you my love and what you feel is most convenient üòö Wishing you sweet dreams and warm wax baths for those soft hands and feet hehe. Goodnight Love! ‚ù§Ô∏èüí§\",\n",
    "    \"To quickly catch you up on the tea (or lack thereof) ü´ñ, I have not spoken to Tanner or many others outside of New York in some time. My strategy is one of humble silence, and until I have fully made back the money lost in 2021 & 2022 I have nothing to brag about ü´£ Afterwards, there is no point in telling others that have not been part of the journey as they will only seek to figure out what benefit they can take from your success. Since they have not taken the time to give back the efforts that I have put into that friendship, instead I will use them as data points for my investments üßê I am planning on keeping track of a little notepad, where I note the date of when someone reaches back out and if they mention crypto. When I get to a point where the rate increases for a period of ~3-6 months, then I know it will be my time to sell ü§≠ As for Ethan, he's doing well and told me of his team dynamics that he is struggling to fit into, because the two other members are Chinese and they speak in Chinese back and forth üò• He is getting the poor reverse foreigner experience competing with the top financial and math talents hehe. Everyone else in general is not experiencing too many major difficulties, just the little hardships with work & the economy that we all feel!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = u.tokenizer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(5,), dtype=string, numpy=array([b'This', b'is', b'the', b'first', b'sentence.'], dtype=object)>, <tf.Tensor: shape=(155,), dtype=string, numpy=\n",
      "array([b'Good', b'evening', b'my', b'love!', b'\\xe2\\x9d\\xa4\\xef\\xb8\\x8f',\n",
      "       b'How', b'difficult', b'it', b'was', b'to', b'leave', b'my',\n",
      "       b'cute', b'and', b'cuddly', b'Pepe', b'Queen', b'this',\n",
      "       b'morning,', b'wanting', b'to', b'crawl', b'under', b'the',\n",
      "       b'covers', b'and', b'KISS', b'KISS', b'her',\n",
      "       b'\\xf0\\x9f\\x98\\x98\\xe2\\x98\\xba\\xef\\xb8\\x8f', b'Her', b'body',\n",
      "       b'still', b'remains', b'in', b'my', b'bed', b'as', b'she', b'has',\n",
      "       b'shed', b'her', b'outfit', b'in', b'the', b'same', b'way',\n",
      "       b'that', b'a', b'snake', b'sheds', b'its', b'skin',\n",
      "       b'\\xf0\\x9f\\xa4\\xad', b'Today', b'was', b'an', b'incredibly',\n",
      "       b'productive', b'day', b'of', b'meetings,', b'to', b'the',\n",
      "       b'point', b'where', b'I', b'have', b'almost', b'lost', b'my',\n",
      "       b'voice', b'\\xf0\\x9f\\x98\\x8b', b'Out', b'of', b'the',\n",
      "       b'conversations,', b'one', b'may', b'even', b'already', b'lead',\n",
      "       b'to', b'an', b'investment', b'I', b'may', b'get', b'personal',\n",
      "       b'access', b'to', b'following', b'the', b'new', b'strategy',\n",
      "       b'discussed', b'yesterday', b'night.', b'Today', b'has', b'also',\n",
      "       b'marked', b'a', b'significantly', b'positive', b'turn', b'in',\n",
      "       b'the', b'market,', b'with', b'Bitcoin', b'up', b'an',\n",
      "       b'unbelievable', b'10%', b'in', b'24', b'hours.', b'I', b'am',\n",
      "       b'sitting', b'patiently', b'upon', b'my', b'tiny', b'digital',\n",
      "       b'empire,', b'watching', b'humbly', b'as', b'my', b'land',\n",
      "       b'blossoms', b'and', b'the', b'world', b'is', b'quiet', b'to',\n",
      "       b'my', b'knowledge', b'and', b'secrets', b'\\xf0\\x9f\\xa4\\xab', b'I',\n",
      "       b'will', b'update', b'you', b'more', b'tomorrow', b'-',\n",
      "       b'goodnight', b'my', b'dear', b'\\xf0\\x9f\\x98\\xb4\\xf0\\x9f\\x92\\x93'],\n",
      "      dtype=object)>, <tf.Tensor: shape=(112,), dtype=string, numpy=\n",
      "array([b'Happy', b'Saturday', b'my', b'beautiful', b'darling!',\n",
      "       b'\\xf0\\x9f\\xa5\\xb9', b'Another', b'crisp', b'and', b'sunny',\n",
      "       b'winter', b'day', b'to', b'spend', b'indoors', b'together', b'in',\n",
      "       b'front', b'of', b'some', b'big', b'screens', b'and', b'each',\n",
      "       b'other', b'\\xe2\\x98\\xba\\xef\\xb8\\x8f', b\"There's\", b'not', b'much',\n",
      "       b'to', b'share', b'on', b'my', b\"cousin's\", b'front,', b'other',\n",
      "       b'than', b'they', b'had', b'been', b'together', b'since', b'2018',\n",
      "       b'which', b'is', b'likely', b'around', b'the', b'last', b'time',\n",
      "       b'I', b'saw', b'him,', b'and', b'she', b'is', b'from', b'a',\n",
      "       b'tiny', b'town', b'in', b'the', b'middle', b'of', b'nowhere',\n",
      "       b'over', b'by', b'him', b'in', b'Michigan.', b'I', b'did',\n",
      "       b'bring', b'up', b'one', b'topic', b'that', b'made', b'her',\n",
      "       b'have', b'an', b'outsized', b'reaction', b'and', b'roll', b'her',\n",
      "       b'eyes', b'which', b'was', b'whether', b'he', b'wanted', b'to',\n",
      "       b'move', b'from', b'Michigan', b'to', b'somewhere', b'else',\n",
      "       b'\\xf0\\x9f\\xa4\\xad', b'I', b'will', b'kiss', b'kiss',\n",
      "       b'\\xf0\\x9f\\x98\\x98', b'and', b'banana', b'banana',\n",
      "       b'\\xf0\\x9f\\x8d\\x8c', b'you', b'soon!',\n",
      "       b'\\xf0\\x9f\\x98\\xb3\\xf0\\x9f\\xab\\xa3'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Awwwwww'], dtype=object)>, <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'I', b'miss', b'you!'], dtype=object)>, <tf.Tensor: shape=(83,), dtype=string, numpy=\n",
      "array([b'Happy', b'Friyay', b'Love!', b'\\xe2\\x9d\\xa4\\xef\\xb8\\x8f',\n",
      "       b'Enjoy', b'your', b'relaxing', b'day', b'at', b'home', b'filled',\n",
      "       b'with', b'lots', b'of', b'destressing', b'and', b'watching',\n",
      "       b'of', b'friends', b'\\xf0\\x9f\\xa4\\x97', b'It', b'would', b'be',\n",
      "       b'wonderful', b'to', b'try', b'out', b'the', b'special', b'egg',\n",
      "       b'yolk', b'one', b'because', b'I', b'am', b'addicted', b'to',\n",
      "       b'being', b'the', b'super', b'special', b'person', b'in', b'your',\n",
      "       b'life', b'\\xf0\\x9f\\xa4\\xad', b'You', b'also', b'have', b'a',\n",
      "       b'card', b'from', b'valentines', b'that', b'I', b'hid', b'in',\n",
      "       b'our', b'shared', b'email', b'inbox', b'for', b'you', b'to',\n",
      "       b'open', b'wuuuuu', b'\\xf0\\x9f\\x98\\x96', b'See', b'you',\n",
      "       b'tomorrow', b'my', b'Beebee', b'and', b'be', b'prepared', b'for',\n",
      "       b'lots', b'more', b'feelings', b'of', b'being', b'loved!',\n",
      "       b'\\xf0\\x9f\\x98\\x98\\xf0\\x9f\\x90\\x9d'], dtype=object)>, <tf.Tensor: shape=(181,), dtype=string, numpy=\n",
      "array([b\"I've\", b'spotted', b'our', b'first', b'day', b'with', b'a',\n",
      "       b'temperature', b'in', b'the', b'60s', b'coming', b'Wednesday',\n",
      "       b'\\xf0\\x9f\\x98\\xb2', b'I', b\"won't\", b'speak', b'too', b'soon',\n",
      "       b'but', b'Mr.', b'Snow', b'may', b'be', b'gone', b'for', b'the',\n",
      "       b'rest', b'of', b'2024', b'\\xf0\\x9f\\x98\\x8e', b'How', b'nice',\n",
      "       b'the', b'dream', b'of', b'living', b'in', b'Miami', b'feels',\n",
      "       b'for', b'half', b'of', b'the', b'year', b'up', b'in', b'the',\n",
      "       b'north!', b'We', b'can', b'fix', b'up', b'your', b'helium',\n",
      "       b'mobile', b'this', b'weekend', b'and', b'get', b'you', b'back',\n",
      "       b'in', b'mapping', b'action', b'earning', b'that', b'stable',\n",
      "       b'income', b'source', b'for', b'the', b'family',\n",
      "       b'\\xf0\\x9f\\xa4\\xad', b\"I'll\", b'be', b'meeting', b'with', b'my',\n",
      "       b'cousin', b'in', b'the', b'afternoon', b'for', b'lunch',\n",
      "       b'tomorrow', b'rather', b'than', b'what', b'I', b'originally',\n",
      "       b'thought', b'would', b'be', b'dinner.', b'And', b'I', b'also',\n",
      "       b'will', b'get', b'a', b'chance', b'to', b'meet', b'his',\n",
      "       b'fiance', b'too', b'so', b'I', b'suppose', b'I', b'will', b'have',\n",
      "       b'to', b'be', b'on', b'my', b'best', b'behavior', b'and', b'see',\n",
      "       b'if', b'there', b'is', b'tea', b'to', b'extract',\n",
      "       b'\\xf0\\x9f\\x98\\xae\\xe2\\x80\\x8d\\xf0\\x9f\\x92\\xa8', b'If', b'you',\n",
      "       b'would', b'like', b'to', b'stay', b'over', b'tomorrow', b'after',\n",
      "       b'your', b'class', b'since', b\"I'll\", b'be', b'gone', b'next',\n",
      "       b'weekend,', b'that', b'is', b'an', b'option!', b'But', b'up',\n",
      "       b'to', b'you', b'my', b'love', b'and', b'what', b'you', b'feel',\n",
      "       b'is', b'most', b'convenient', b'\\xf0\\x9f\\x98\\x9a', b'Wishing',\n",
      "       b'you', b'sweet', b'dreams', b'and', b'warm', b'wax', b'baths',\n",
      "       b'for', b'those', b'soft', b'hands', b'and', b'feet', b'hehe.',\n",
      "       b'Goodnight', b'Love!',\n",
      "       b'\\xe2\\x9d\\xa4\\xef\\xb8\\x8f\\xf0\\x9f\\x92\\xa4'], dtype=object)>, <tf.Tensor: shape=(249,), dtype=string, numpy=\n",
      "array([b'To', b'quickly', b'catch', b'you', b'up', b'on', b'the', b'tea',\n",
      "       b'(or', b'lack', b'thereof)', b'\\xf0\\x9f\\xab\\x96,', b'I', b'have',\n",
      "       b'not', b'spoken', b'to', b'Tanner', b'or', b'many', b'others',\n",
      "       b'outside', b'of', b'New', b'York', b'in', b'some', b'time.',\n",
      "       b'My', b'strategy', b'is', b'one', b'of', b'humble', b'silence,',\n",
      "       b'and', b'until', b'I', b'have', b'fully', b'made', b'back',\n",
      "       b'the', b'money', b'lost', b'in', b'2021', b'&', b'2022', b'I',\n",
      "       b'have', b'nothing', b'to', b'brag', b'about', b'\\xf0\\x9f\\xab\\xa3',\n",
      "       b'Afterwards,', b'there', b'is', b'no', b'point', b'in',\n",
      "       b'telling', b'others', b'that', b'have', b'not', b'been', b'part',\n",
      "       b'of', b'the', b'journey', b'as', b'they', b'will', b'only',\n",
      "       b'seek', b'to', b'figure', b'out', b'what', b'benefit', b'they',\n",
      "       b'can', b'take', b'from', b'your', b'success.', b'Since', b'they',\n",
      "       b'have', b'not', b'taken', b'the', b'time', b'to', b'give',\n",
      "       b'back', b'the', b'efforts', b'that', b'I', b'have', b'put',\n",
      "       b'into', b'that', b'friendship,', b'instead', b'I', b'will',\n",
      "       b'use', b'them', b'as', b'data', b'points', b'for', b'my',\n",
      "       b'investments', b'\\xf0\\x9f\\xa7\\x90', b'I', b'am', b'planning',\n",
      "       b'on', b'keeping', b'track', b'of', b'a', b'little', b'notepad,',\n",
      "       b'where', b'I', b'note', b'the', b'date', b'of', b'when',\n",
      "       b'someone', b'reaches', b'back', b'out', b'and', b'if', b'they',\n",
      "       b'mention', b'crypto.', b'When', b'I', b'get', b'to', b'a',\n",
      "       b'point', b'where', b'the', b'rate', b'increases', b'for', b'a',\n",
      "       b'period', b'of', b'~3-6', b'months,', b'then', b'I', b'know',\n",
      "       b'it', b'will', b'be', b'my', b'time', b'to', b'sell',\n",
      "       b'\\xf0\\x9f\\xa4\\xad', b'As', b'for', b'Ethan,', b\"he's\", b'doing',\n",
      "       b'well', b'and', b'told', b'me', b'of', b'his', b'team',\n",
      "       b'dynamics', b'that', b'he', b'is', b'struggling', b'to', b'fit',\n",
      "       b'into,', b'because', b'the', b'two', b'other', b'members', b'are',\n",
      "       b'Chinese', b'and', b'they', b'speak', b'in', b'Chinese', b'back',\n",
      "       b'and', b'forth', b'\\xf0\\x9f\\x98\\xa5', b'He', b'is', b'getting',\n",
      "       b'the', b'poor', b'reverse', b'foreigner', b'experience',\n",
      "       b'competing', b'with', b'the', b'top', b'financial', b'and',\n",
      "       b'math', b'talents', b'hehe.', b'Everyone', b'else', b'in',\n",
      "       b'general', b'is', b'not', b'experiencing', b'too', b'many',\n",
      "       b'major', b'difficulties,', b'just', b'the', b'little',\n",
      "       b'hardships', b'with', b'work', b'&', b'the', b'economy', b'that',\n",
      "       b'we', b'all', b'feel!'], dtype=object)>]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab_size = u.vocabularyBuilder(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.preprocessing.text.Tokenizer object at 0x12ff673d0>\n",
      "426\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = u.converter(vocab, tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35, 9, 2, 52, 97], [98, 99, 7, 36, 53, 54, 100, 37, 38, 3, 101, 7, 102, 5, 103, 104, 105, 35, 106, 107, 3, 108, 109, 2, 110, 5, 27, 27, 20, 111, 20, 112, 113, 114, 6, 7, 115, 21, 55, 56, 116, 20, 117, 6, 2, 118, 119, 11, 14, 120, 121, 122, 123, 22, 57, 38, 23, 124, 125, 28, 8, 126, 3, 2, 39, 40, 4, 12, 127, 58, 7, 128, 129, 29, 8, 2, 130, 30, 41, 131, 132, 133, 3, 23, 134, 4, 41, 31, 135, 136, 3, 137, 2, 59, 60, 138, 139, 140, 57, 56, 42, 141, 14, 142, 143, 144, 6, 2, 145, 17, 146, 18, 23, 147, 148, 6, 149, 150, 4, 43, 151, 152, 153, 7, 61, 154, 155, 62, 156, 21, 7, 157, 158, 5, 2, 159, 9, 160, 3, 7, 161, 5, 162, 163, 4, 16, 164, 10, 63, 32, 165, 64, 7, 166, 167], [65, 168, 7, 169, 170, 171, 172, 173, 5, 174, 175, 28, 3, 176, 177, 66, 6, 178, 8, 67, 179, 180, 5, 181, 44, 182, 183, 24, 184, 3, 185, 33, 7, 186, 187, 44, 68, 19, 188, 69, 66, 45, 189, 70, 9, 190, 191, 2, 192, 46, 4, 193, 194, 5, 55, 9, 34, 14, 61, 195, 6, 2, 196, 8, 197, 71, 198, 199, 6, 200, 4, 201, 202, 18, 30, 203, 11, 72, 20, 12, 23, 204, 205, 5, 206, 20, 207, 70, 38, 208, 47, 209, 3, 210, 34, 211, 3, 212, 73, 22, 4, 16, 27, 27, 213, 5, 74, 74, 214, 10, 215, 216], [217], [4, 218, 219], [65, 220, 36, 53, 221, 25, 222, 28, 223, 224, 225, 17, 75, 8, 226, 5, 62, 8, 227, 228, 37, 48, 15, 229, 3, 230, 29, 2, 76, 231, 232, 30, 77, 4, 43, 233, 3, 78, 2, 234, 76, 235, 6, 25, 236, 22, 10, 42, 12, 14, 237, 34, 238, 11, 4, 239, 6, 79, 240, 241, 242, 13, 10, 3, 243, 244, 245, 80, 10, 32, 7, 246, 5, 15, 247, 13, 75, 63, 248, 8, 78, 249, 250], [251, 252, 79, 52, 28, 17, 14, 253, 6, 2, 254, 255, 256, 257, 4, 258, 81, 49, 259, 82, 260, 261, 41, 15, 83, 13, 2, 262, 8, 263, 264, 54, 265, 2, 266, 8, 267, 6, 268, 269, 13, 270, 8, 2, 271, 18, 6, 2, 272, 84, 85, 273, 18, 25, 274, 275, 35, 276, 5, 31, 10, 26, 6, 277, 278, 279, 11, 280, 281, 282, 13, 2, 283, 22, 86, 15, 284, 17, 7, 285, 6, 2, 286, 13, 287, 32, 288, 68, 50, 4, 289, 290, 48, 15, 291, 5, 4, 42, 16, 31, 14, 292, 3, 293, 87, 294, 49, 295, 4, 296, 4, 16, 12, 3, 15, 33, 7, 297, 298, 5, 80, 51, 88, 9, 89, 3, 299, 300, 51, 10, 48, 301, 3, 302, 71, 32, 303, 25, 304, 45, 86, 15, 83, 305, 306, 11, 9, 23, 307, 82, 18, 3, 10, 7, 308, 5, 50, 10, 309, 9, 310, 311, 312, 313, 10, 314, 315, 5, 316, 317, 318, 13, 319, 320, 321, 5, 322, 90, 64, 36, 323], [3, 324, 325, 10, 18, 33, 2, 89, 326, 327, 328, 329, 4, 12, 24, 330, 3, 331, 332, 91, 92, 333, 8, 59, 334, 6, 67, 335, 7, 60, 9, 30, 8, 336, 337, 5, 338, 4, 12, 339, 72, 26, 2, 340, 58, 6, 341, 93, 342, 4, 12, 343, 3, 344, 345, 346, 347, 88, 9, 348, 39, 6, 349, 92, 11, 12, 24, 69, 350, 8, 2, 351, 21, 19, 16, 352, 353, 3, 354, 29, 50, 355, 19, 85, 356, 34, 25, 357, 45, 19, 12, 24, 358, 2, 46, 3, 359, 26, 2, 360, 11, 4, 12, 361, 362, 11, 363, 364, 4, 16, 365, 366, 21, 367, 368, 13, 7, 369, 370, 4, 43, 371, 33, 372, 373, 8, 14, 94, 374, 40, 4, 375, 2, 376, 8, 95, 377, 378, 26, 29, 5, 51, 19, 379, 380, 95, 4, 31, 3, 14, 39, 40, 2, 381, 382, 13, 14, 383, 8, 384, 385, 386, 4, 387, 37, 16, 15, 7, 46, 3, 388, 22, 21, 13, 389, 390, 391, 392, 5, 393, 394, 8, 87, 395, 396, 11, 47, 9, 397, 3, 398, 399, 77, 2, 400, 44, 401, 402, 96, 5, 19, 81, 6, 96, 26, 5, 403, 404, 47, 9, 405, 2, 406, 407, 408, 409, 410, 17, 2, 411, 412, 5, 413, 414, 90, 415, 73, 6, 416, 9, 24, 417, 49, 91, 418, 419, 420, 2, 94, 421, 17, 422, 93, 2, 423, 11, 84, 424, 425]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences, output_sequences = u.preprecessData(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 35   9   2 ...   0   0   0]\n",
      " [ 98  99   7 ...   0   0   0]\n",
      " [ 65 168   7 ...   0   0   0]\n",
      " ...\n",
      " [ 65 220  36 ...   0   0   0]\n",
      " [251 252  79 ...   0   0   0]\n",
      " [  3 324 325 ...  84 424   0]] (8, 249)\n",
      "[[  9   2  52 ...   0   0   0]\n",
      " [ 99   7  36 ...   0   0   0]\n",
      " [168   7 169 ...   0   0   0]\n",
      " ...\n",
      " [220  36  53 ...   0   0   0]\n",
      " [252  79  52 ...   0   0   0]\n",
      " [324 325  10 ... 424 425   0]] (8, 249)\n"
     ]
    }
   ],
   "source": [
    "print(input_sequences, input_sequences.shape)\n",
    "print(output_sequences, output_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = u.buildModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = u.compileModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7134 - accuracy: 0.6135\n"
     ]
    }
   ],
   "source": [
    "history = u.fitModel(model, input_sequences, output_sequences, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "result = u.generateText(model, vocab, seed_text=\"You are\", max_length=250, temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> not may <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> they <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> i <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> the <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> nowhere <OOV> <OOV> <OOV> <OOV> <OOV> town <OOV> <OOV> they <OOV> <OOV> <OOV> <OOV> to <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> of card <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> personal <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> saw <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> beebee <OOV> <OOV> <OOV> <OOV> <OOV> for <OOV> <OOV> <OOV> <OOV> she <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> rather <OOV> <OOV> <OOV> <OOV> since <OOV> <OOV> <OOV> an <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> knowledge <OOV> <OOV> in üòò‚ò∫Ô∏è <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> as <OOV> <OOV> <OOV> money <OOV> <OOV> <OOV> <OOV> <OOV> because <OOV> <OOV> <OOV> for <OOV> saw <OOV> beebee <OOV> <OOV> time <OOV> i <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> one <OOV> <OOV> <OOV> bring\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
